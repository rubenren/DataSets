{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d532cab",
   "metadata": {},
   "source": [
    "I want to try out multiclass, and see if I get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86d76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025ed79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for one of our technical indicators\n",
    "def calc_rsi(df, periods = 14, ema = True):\n",
    "    \"\"\"\n",
    "    Returns a pd.Series with the relative strength index.\n",
    "    \"\"\"\n",
    "    close_delta = df.adj_close.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "        # Use exponential moving average\n",
    "        ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "        ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ebe342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining our preprocessing\n",
    "def read_data(filename):\n",
    "    # read in the data\n",
    "    data = pd.read_csv(filename)\n",
    "    data = data.rename(str.lower, axis=1)\n",
    "    data = data.rename(mapper={'adj close': 'adj_close'}, axis=1)\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "    data = data.set_index('date')\n",
    "    return data\n",
    "\n",
    "def add_features(df):\n",
    "    # accumulation/Distribution line\n",
    "    mult = ((df.close - df.low) - (df.high - df.close)) / (df.high - df.low)\n",
    "    MFVolume = mult * df.volume\n",
    "    accum_dist_indicator = MFVolume.cumsum()\n",
    "    ret_df = pd.concat([df, accum_dist_indicator], axis=1)\n",
    "    ret_df = ret_df.rename(mapper={0:'accum_dist_indicator'}, axis=1)\n",
    "    \n",
    "    #MACD\n",
    "    EMA_12 = df.adj_close.ewm(span=12, adjust=False).mean()\n",
    "    EMA_26 = df.adj_close.ewm(span=26, adjust=False).mean()\n",
    "    macd = EMA_12 - EMA_26\n",
    "    signal = macd.ewm(span=9, adjust=False).mean()\n",
    "    ret_df = pd.concat([ret_df, macd.rename('macd'), signal.rename('signal_macd')], axis=1)\n",
    "    \n",
    "    #RSI\n",
    "    rsi = calc_rsi(df)\n",
    "    ret_df = pd.concat([ret_df, rsi.rename('rsi')], axis=1)\n",
    "    \n",
    "    return ret_df\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, col_names = [], indicies = [], preds = [], dropnan=True):\n",
    "    '''\n",
    "    Convert a time series to a supervised learning dataset\n",
    "    Args:\n",
    "        data -> time series to convert as a list or numpy array\n",
    "        n_in -> number of lag observations as input (X)\n",
    "        n_out -> number of observations as output (y)\n",
    "        col_names -> names of the columns\n",
    "        indicies -> list of the indicies\n",
    "        preds -> list of column indicies to determine which variables to predict\n",
    "        dropnan -> flag of whether to drop the rows with NaN\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning\n",
    "    '''\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s(t-%d)' % (col_names[j], i)) for j in range(n_vars)]\n",
    "    # forecast sequence\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df[col_names[preds]].shift(-i))\n",
    "        if i==0:\n",
    "            names += [('%s(t)' % (col_names[j])) for j in preds]\n",
    "        else:\n",
    "            names += [('%s(t+%d)' % (col_names[j], i)) for j in preds]\n",
    "    # putting it together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    agg.index = indicies\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def scale(in_train, in_test, with_labels=False):\n",
    "    '''\n",
    "    Rescales the train and test sets\n",
    "    Args:\n",
    "        train -> numpy array of the training data\n",
    "        test -> numpy array of the test data\n",
    "        with_labels -> if set to true will cut off last column before scaling,\n",
    "            reattached after scaling\n",
    "    Returns:\n",
    "        scaler -> the scaler object for transforming\n",
    "        train_scaled -> a rescaled version of the train data\n",
    "        test_scaled -> a rescaled version of the test data\n",
    "    '''\n",
    "    train = in_train\n",
    "    test = in_test\n",
    "    if with_labels:\n",
    "        train_labels = train[:,-1]\n",
    "        train = train[:,:-1]\n",
    "        test_labels = test[:,-1]\n",
    "        test = test[:,:-1]\n",
    "    # scale train and test to [-1,1]\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    if with_labels:\n",
    "        train_scaled.append(train_labels, axis=1)\n",
    "        test_scaled.append(test_labels, axis=1)\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "def split_data(data, test_percent):\n",
    "    '''\n",
    "    Splits the data by percentage amount\n",
    "    Returns: train, test\n",
    "    '''\n",
    "    split_val = int(len(data) * (1 - test_percent))\n",
    "    train, test = data[:split_val], data[split_val:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9937be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_separation(x):\n",
    "    if x < -.05:\n",
    "        return -2\n",
    "    elif x < -.005:\n",
    "        return -1\n",
    "    elif x > .05:\n",
    "        return 2\n",
    "    elif x > .005:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def prep_data(filename, lookback=14):\n",
    "    temp = np.array([0,1,2,-1,-2]).reshape(-1,1)\n",
    "    # read in the data\n",
    "    data = read_data(filename)\n",
    "    # add technical indicators as features\n",
    "    data = add_features(data)\n",
    "    # frame as an RNN problem\n",
    "    data = series_to_supervised(data, lookback, 5, data.columns, data.index, [4])\n",
    "    # only keep the predictive columns I care about\n",
    "    data = data.drop(data.columns[-5:-1], axis=1)\n",
    "    labels = (data['adj_close(t+4)'] - data['adj_close(t-1)']) / data['adj_close(t-1)']\n",
    "    labels = labels.apply(class_separation)\n",
    "    data = data.drop(data.columns[-1], axis=1)\n",
    "    data = pd.concat([data, labels.rename('labels')], axis=1)\n",
    "    data_values = data.values\n",
    "    train, test = split_data(data_values, .2)\n",
    "    scaler, train_scaled, test_scaled = scale(train[:,:-1], test[:,:-1])\n",
    "    ohe = OneHotEncoder(sparse=False).fit(temp)\n",
    "    train_scaled = np.append(train_scaled, ohe.transform(train[:,-1].reshape((-1,1))), axis=1)\n",
    "    test_scaled = np.append(test_scaled, ohe.transform(test[:,-1].reshape((-1,1))), axis=1)\n",
    "    return scaler, train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92b3ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, batch_size, nb_epoch, neurons, label_size=5):\n",
    "    X, y = train[:,:-label_size], train[:,-label_size:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # prepare a model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True, return_sequences=True))\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'AUC'])\n",
    "    for i in range(nb_epoch):\n",
    "        if not i % 10:\n",
    "            print('%d/%d' % (i+1, nb_epoch), end='')\n",
    "        else:\n",
    "            print('.', end='')\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "    print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e4cb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - 8s 6ms/step - loss: 1.2322 - accuracy: 0.4367 - auc: 0.7828\n",
      "370/370 [==============================] - 6s 6ms/step - loss: 1.2088 - accuracy: 0.4377 - auc: 0.7888\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1976 - accuracy: 0.4397 - auc: 0.7917\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1913 - accuracy: 0.4341 - auc: 0.7928\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1808 - accuracy: 0.4350 - auc: 0.7956\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1718 - accuracy: 0.4390 - auc: 0.7985\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1693 - accuracy: 0.4444 - auc: 0.7995\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1657 - accuracy: 0.4426 - auc: 0.8010\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1628 - accuracy: 0.4505 - auc: 0.8020\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.1581 - accuracy: 0.4500 - auc: 0.8034\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1550 - accuracy: 0.4510 - auc: 0.8045\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1501 - accuracy: 0.4559 - auc: 0.8062\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1460 - accuracy: 0.4566 - auc: 0.8081\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.1407 - accuracy: 0.4639 - auc: 0.8098\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1366 - accuracy: 0.4622 - auc: 0.8111\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.1336 - accuracy: 0.4610 - auc: 0.8125\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1333 - accuracy: 0.4622 - auc: 0.8124\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1314 - accuracy: 0.4650 - auc: 0.8132\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1304 - accuracy: 0.4688 - auc: 0.8129\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1192 - accuracy: 0.4747 - auc: 0.8170\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1141 - accuracy: 0.4779 - auc: 0.8192\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1106 - accuracy: 0.4774 - auc: 0.8200\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1067 - accuracy: 0.4841 - auc: 0.8220\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.1019 - accuracy: 0.4880 - auc: 0.8242\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0958 - accuracy: 0.4851 - auc: 0.8263\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.0980 - accuracy: 0.4801 - auc: 0.8252\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0905 - accuracy: 0.4892 - auc: 0.8288\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0798 - accuracy: 0.4946 - auc: 0.8322\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.0788 - accuracy: 0.4985 - auc: 0.8331\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0726 - accuracy: 0.4949 - auc: 0.8339\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0654 - accuracy: 0.5039 - auc: 0.8373\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0734 - accuracy: 0.4924 - auc: 0.8341\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.0620 - accuracy: 0.5088 - auc: 0.8387\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 1.0466 - accuracy: 0.5130 - auc: 0.8439\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0516 - accuracy: 0.5218 - auc: 0.8428\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 1.0455 - accuracy: 0.5198 - auc: 0.8445\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0405 - accuracy: 0.5264 - auc: 0.8460\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0239 - accuracy: 0.5279 - auc: 0.8513\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.0187 - accuracy: 0.5299 - auc: 0.8525\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0173 - accuracy: 0.5395 - auc: 0.8537\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 1.0138 - accuracy: 0.5265 - auc: 0.8544\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0103 - accuracy: 0.5291 - auc: 0.8548\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 1.0081 - accuracy: 0.5319 - auc: 0.8562\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9916 - accuracy: 0.5395 - auc: 0.8613\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.9888 - accuracy: 0.5475 - auc: 0.8616\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9800 - accuracy: 0.5497 - auc: 0.8657\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9814 - accuracy: 0.5551 - auc: 0.8641\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9725 - accuracy: 0.5557 - auc: 0.8673\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9627 - accuracy: 0.5557 - auc: 0.8701\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9543 - accuracy: 0.5600 - auc: 0.8715\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.9482 - accuracy: 0.5720 - auc: 0.8748\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9496 - accuracy: 0.5657 - auc: 0.8731\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9371 - accuracy: 0.5747 - auc: 0.8774\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9317 - accuracy: 0.5762 - auc: 0.8789\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9260 - accuracy: 0.5819 - auc: 0.8804\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9398 - accuracy: 0.5640 - auc: 0.8753\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9545 - accuracy: 0.5691 - auc: 0.8731\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9226 - accuracy: 0.5774 - auc: 0.8804\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.9164 - accuracy: 0.5792 - auc: 0.8825\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9129 - accuracy: 0.5814 - auc: 0.8839\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9083 - accuracy: 0.5873 - auc: 0.8840\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8973 - accuracy: 0.5917 - auc: 0.8876\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8879 - accuracy: 0.5917 - auc: 0.8902\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9025 - accuracy: 0.5853 - auc: 0.8861\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8877 - accuracy: 0.6002 - auc: 0.8898\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.8694 - accuracy: 0.6090 - auc: 0.8951\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8717 - accuracy: 0.6042 - auc: 0.8936\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8616 - accuracy: 0.6172 - auc: 0.8973\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8689 - accuracy: 0.6093 - auc: 0.8950\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8691 - accuracy: 0.6073 - auc: 0.8945\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8676 - accuracy: 0.5990 - auc: 0.8943\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8478 - accuracy: 0.6166 - auc: 0.9002\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8494 - accuracy: 0.6144 - auc: 0.9004\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.9220 - accuracy: 0.5833 - auc: 0.8822\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.8767 - accuracy: 0.6034 - auc: 0.8924\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8765 - accuracy: 0.6066 - auc: 0.8938\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8298 - accuracy: 0.6262 - auc: 0.9052\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8349 - accuracy: 0.6213 - auc: 0.9036\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8157 - accuracy: 0.6387 - auc: 0.9084\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.8202 - accuracy: 0.6289 - auc: 0.9071\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8097 - accuracy: 0.6328 - auc: 0.9093\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.8046 - accuracy: 0.6336 - auc: 0.9106\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7899 - accuracy: 0.6481 - auc: 0.9148\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8023 - accuracy: 0.6443 - auc: 0.9114\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8069 - accuracy: 0.6367 - auc: 0.9097\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.8103 - accuracy: 0.6400 - auc: 0.9089\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.7967 - accuracy: 0.6372 - auc: 0.9122\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8167 - accuracy: 0.6280 - auc: 0.9073\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7912 - accuracy: 0.6417 - auc: 0.9134\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7853 - accuracy: 0.6443 - auc: 0.9148\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.7829 - accuracy: 0.6471 - auc: 0.9154\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.7663 - accuracy: 0.6527 - auc: 0.9197\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7803 - accuracy: 0.6495 - auc: 0.9157\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8106 - accuracy: 0.6340 - auc: 0.9090\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8752 - accuracy: 0.6034 - auc: 0.8944\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.8110 - accuracy: 0.6316 - auc: 0.9083\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7986 - accuracy: 0.6476 - auc: 0.9128\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7801 - accuracy: 0.6522 - auc: 0.9164\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7646 - accuracy: 0.6568 - auc: 0.9203\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7500 - accuracy: 0.6684 - auc: 0.9235\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.7602 - accuracy: 0.6639 - auc: 0.9209\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7668 - accuracy: 0.6529 - auc: 0.9186\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7487 - accuracy: 0.6698 - auc: 0.9234\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7361 - accuracy: 0.6743 - auc: 0.9259\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.7515 - accuracy: 0.6655 - auc: 0.9225\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.7547 - accuracy: 0.6590 - auc: 0.9219\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.7409 - accuracy: 0.6760 - auc: 0.9248\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7330 - accuracy: 0.6709 - auc: 0.9262\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7424 - accuracy: 0.6725 - auc: 0.9248\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7643 - accuracy: 0.6578 - auc: 0.9196\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7430 - accuracy: 0.6696 - auc: 0.9245\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7262 - accuracy: 0.6787 - auc: 0.9280\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7181 - accuracy: 0.6868 - auc: 0.9302\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7156 - accuracy: 0.6855 - auc: 0.9305\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.6894 - accuracy: 0.6966 - auc: 0.9358\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7228 - accuracy: 0.6782 - auc: 0.9283\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.7191 - accuracy: 0.6809 - auc: 0.9293\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6849 - accuracy: 0.6963 - auc: 0.9368\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6881 - accuracy: 0.7039 - auc: 0.9361\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6695 - accuracy: 0.7057 - auc: 0.9398\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6907 - accuracy: 0.6943 - auc: 0.9351\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6767 - accuracy: 0.7056 - auc: 0.9384\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6716 - accuracy: 0.7051 - auc: 0.9388\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.6694 - accuracy: 0.7056 - auc: 0.9393\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6847 - accuracy: 0.7017 - auc: 0.9362\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6692 - accuracy: 0.7083 - auc: 0.9394\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6740 - accuracy: 0.7032 - auc: 0.9383\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6885 - accuracy: 0.6990 - auc: 0.9353\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6694 - accuracy: 0.7084 - auc: 0.9393\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6629 - accuracy: 0.7088 - auc: 0.9405\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6830 - accuracy: 0.7010 - auc: 0.9365\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6837 - accuracy: 0.6985 - auc: 0.9362\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6520 - accuracy: 0.7160 - auc: 0.9428\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6387 - accuracy: 0.7209 - auc: 0.9454\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6331 - accuracy: 0.7240 - auc: 0.9463\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6477 - accuracy: 0.7172 - auc: 0.9432\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6529 - accuracy: 0.7123 - auc: 0.9420\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6357 - accuracy: 0.7218 - auc: 0.9453\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6449 - accuracy: 0.7164 - auc: 0.9435\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6281 - accuracy: 0.7221 - auc: 0.9466\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6295 - accuracy: 0.7275 - auc: 0.9465\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6316 - accuracy: 0.7218 - auc: 0.9462\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6405 - accuracy: 0.7177 - auc: 0.9443\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6336 - accuracy: 0.7211 - auc: 0.9458\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6243 - accuracy: 0.7255 - auc: 0.9475\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6501 - accuracy: 0.7123 - auc: 0.9425\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6352 - accuracy: 0.7284 - auc: 0.9458\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.6087 - accuracy: 0.7448 - auc: 0.9507\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6237 - accuracy: 0.7309 - auc: 0.9475\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6134 - accuracy: 0.7250 - auc: 0.9491\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6068 - accuracy: 0.7307 - auc: 0.9502\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.6016 - accuracy: 0.7407 - auc: 0.9513\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5974 - accuracy: 0.7421 - auc: 0.9521\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6214 - accuracy: 0.7301 - auc: 0.9475\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.6417 - accuracy: 0.7220 - auc: 0.9443\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.5975 - accuracy: 0.7426 - auc: 0.9520\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5784 - accuracy: 0.7549 - auc: 0.9555\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5836 - accuracy: 0.7476 - auc: 0.9544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5765 - accuracy: 0.7429 - auc: 0.9554\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5679 - accuracy: 0.7551 - auc: 0.9571\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5759 - accuracy: 0.7478 - auc: 0.9556\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6036 - accuracy: 0.7336 - auc: 0.9504\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6000 - accuracy: 0.7326 - auc: 0.9512\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5944 - accuracy: 0.7434 - auc: 0.9524\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5978 - accuracy: 0.7429 - auc: 0.9517\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5724 - accuracy: 0.7495 - auc: 0.9558\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5694 - accuracy: 0.7532 - auc: 0.9566\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6098 - accuracy: 0.7319 - auc: 0.9492\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.6179 - accuracy: 0.7236 - auc: 0.9479\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5844 - accuracy: 0.7466 - auc: 0.9538\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.5713 - accuracy: 0.7508 - auc: 0.9565\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5784 - accuracy: 0.7507 - auc: 0.9549\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5596 - accuracy: 0.7554 - auc: 0.9580\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.5771 - accuracy: 0.7530 - auc: 0.9553\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5811 - accuracy: 0.7485 - auc: 0.9545\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5644 - accuracy: 0.7554 - auc: 0.9570\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5873 - accuracy: 0.7468 - auc: 0.9532\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5646 - accuracy: 0.7554 - auc: 0.9573\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5779 - accuracy: 0.7498 - auc: 0.9551\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5357 - accuracy: 0.7720 - auc: 0.9615\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5801 - accuracy: 0.7481 - auc: 0.9546\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5681 - accuracy: 0.7532 - auc: 0.9564\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5617 - accuracy: 0.7551 - auc: 0.9575\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5522 - accuracy: 0.7628 - auc: 0.9590\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5474 - accuracy: 0.7642 - auc: 0.9597\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5477 - accuracy: 0.7667 - auc: 0.9597\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5610 - accuracy: 0.7650 - auc: 0.9579\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5378 - accuracy: 0.7650 - auc: 0.9612\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5246 - accuracy: 0.7706 - auc: 0.9632\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5231 - accuracy: 0.7774 - auc: 0.9634\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5710 - accuracy: 0.7546 - auc: 0.9559\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5323 - accuracy: 0.7758 - auc: 0.9621\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5365 - accuracy: 0.7735 - auc: 0.9612\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5214 - accuracy: 0.7720 - auc: 0.9633\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5219 - accuracy: 0.7743 - auc: 0.9634\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5313 - accuracy: 0.7740 - auc: 0.9623\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5152 - accuracy: 0.7752 - auc: 0.9644\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5196 - accuracy: 0.7735 - auc: 0.9637\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5433 - accuracy: 0.7606 - auc: 0.9598\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5431 - accuracy: 0.7642 - auc: 0.9600\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5476 - accuracy: 0.7635 - auc: 0.9595\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5286 - accuracy: 0.7689 - auc: 0.9621\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5544 - accuracy: 0.7569 - auc: 0.9581\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5306 - accuracy: 0.7728 - auc: 0.9618\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5114 - accuracy: 0.7812 - auc: 0.9651\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4900 - accuracy: 0.7953 - auc: 0.9684\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4866 - accuracy: 0.7934 - auc: 0.9687\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5497 - accuracy: 0.7650 - auc: 0.9592\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5359 - accuracy: 0.7723 - auc: 0.9612\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5360 - accuracy: 0.7701 - auc: 0.9610\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5152 - accuracy: 0.7769 - auc: 0.9641\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4991 - accuracy: 0.7885 - auc: 0.9668\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4761 - accuracy: 0.8027 - auc: 0.9702\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.5085 - accuracy: 0.7848 - auc: 0.9653\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4781 - accuracy: 0.7971 - auc: 0.9697\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5377 - accuracy: 0.7649 - auc: 0.9606\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4983 - accuracy: 0.7875 - auc: 0.9666\n",
      "370/370 [==============================] - 3s 6ms/step - loss: 0.4873 - accuracy: 0.7917 - auc: 0.9681\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4828 - accuracy: 0.8020 - auc: 0.9688\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4701 - accuracy: 0.8025 - auc: 0.9706\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4923 - accuracy: 0.7909 - auc: 0.9674\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4888 - accuracy: 0.7917 - auc: 0.9680\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5080 - accuracy: 0.7848 - auc: 0.9650\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4887 - accuracy: 0.7919 - auc: 0.9678\n",
      "370/370 [==============================] - 4s 6ms/step - loss: 0.4649 - accuracy: 0.8005 - auc: 0.9713\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4612 - accuracy: 0.8037 - auc: 0.9721\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4551 - accuracy: 0.8118 - auc: 0.9726\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4839 - accuracy: 0.7910 - auc: 0.9683\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4847 - accuracy: 0.7985 - auc: 0.9684\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5054 - accuracy: 0.7772 - auc: 0.9651\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.5242 - accuracy: 0.7748 - auc: 0.9626\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4980 - accuracy: 0.7861 - auc: 0.9663\n",
      "370/370 [==============================] - 5s 8ms/step - loss: 0.4882 - accuracy: 0.7944 - auc: 0.9680\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4841 - accuracy: 0.7959 - auc: 0.9685\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4692 - accuracy: 0.8037 - auc: 0.9704\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4438 - accuracy: 0.8142 - auc: 0.9740\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4633 - accuracy: 0.8035 - auc: 0.9713\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4397 - accuracy: 0.8150 - auc: 0.9746\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4575 - accuracy: 0.8034 - auc: 0.9718\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4635 - accuracy: 0.8019 - auc: 0.9709\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4824 - accuracy: 0.7939 - auc: 0.9685\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4800 - accuracy: 0.7951 - auc: 0.9689\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4321 - accuracy: 0.8198 - auc: 0.9755\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4136 - accuracy: 0.8324 - auc: 0.9779\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4322 - accuracy: 0.8166 - auc: 0.9753\n",
      "370/370 [==============================] - 4s 8ms/step - loss: 0.4270 - accuracy: 0.8228 - auc: 0.9760\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4705 - accuracy: 0.8041 - auc: 0.9704\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4912 - accuracy: 0.7929 - auc: 0.9673\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.4741 - accuracy: 0.8042 - auc: 0.9698\n",
      "370/370 [==============================] - 4s 7ms/step - loss: 0.5092 - accuracy: 0.7853 - auc: 0.9650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "scaler, train, test = prep_data('../../data/SPY_1993-01-29_2022-08-17.csv')\n",
    "# set parameters\n",
    "batches = 16\n",
    "neurons = 32\n",
    "nb_epochs = 250\n",
    "#trim the data and give it to the model\n",
    "train_trimmed = train[len(train)%batches:]\n",
    "test_trimmed = test[:-(len(test) % batches)] if len(test) % batches != 0 else test\n",
    "lstm_model = fit_lstm(train_trimmed, batches, nb_epochs, neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2b0e48d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - 5s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.2077448e-06, 2.2671573e-02, 9.3674004e-01, 4.0555473e-02,\n",
       "        3.1706568e-05],\n",
       "       [2.2110155e-06, 3.8984701e-02, 9.1802424e-01, 4.2956367e-02,\n",
       "        3.2455700e-05],\n",
       "       [1.4705214e-06, 6.8180434e-02, 8.4498042e-01, 8.6601466e-02,\n",
       "        2.3623864e-04],\n",
       "       [4.0483943e-07, 3.2408819e-02, 8.8286209e-01, 8.4544219e-02,\n",
       "        1.8435836e-04],\n",
       "       [5.5589896e-07, 3.2389209e-02, 7.5076908e-01, 2.1659625e-01,\n",
       "        2.4494572e-04]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = train_trimmed[:,:-5], train_trimmed[:,-5:]\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "y_pred = lstm_model.predict(X, batch_size=16)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32ed26f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - 6s 5ms/step - loss: 0.7436 - accuracy: 0.7073 - auc: 0.9347\n"
     ]
    }
   ],
   "source": [
    "lstm_model.reset_states()\n",
    "metrics = lstm_model.evaluate(X, y, batch_size=batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4fd6e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy', 'auc']\n",
      "[0.7435736656188965, 0.7072635293006897, 0.9346805214881897]\n"
     ]
    }
   ],
   "source": [
    "print(lstm_model.metrics_names)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4417d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 2s 3ms/step\n",
      "92/92 [==============================] - 2s 6ms/step - loss: 3.0207 - accuracy: 0.2833 - auc: 0.6445\n"
     ]
    }
   ],
   "source": [
    "lstm_model.reset_states()\n",
    "lstm_model.predict(X, batch_size=16)\n",
    "X, y = test_trimmed[:,:-5], test_trimmed[:,-5:]\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "metrics, something, sss = lstm_model.evaluate(X,y, batch_size=batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b7bd445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy', 'auc']\n",
      "[3.1463370323181152, 0.2805706560611725, 0.6415072083473206]\n"
     ]
    }
   ],
   "source": [
    "print(lstm_model.metrics_names)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94320791",
   "metadata": {},
   "source": [
    "Overfit pretty well, now time to experiment with different dropout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7190f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, batch_size, nb_epoch, neurons, dropout=.2, label_size=5):\n",
    "    X, y = train[:,:-label_size], train[:,-label_size:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # prepare a model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True, return_sequences=True, dropout=dropout))\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True, dropout=dropout))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Precision(),\n",
    "                                                                              tf.keras.metrics.Recall(),\n",
    "                                                                              'AUC'])\n",
    "    for i in range(nb_epoch):\n",
    "        if not i % 10:\n",
    "            print('%d/%d' % (i+1, nb_epoch), end='')\n",
    "        else:\n",
    "            print('.', end='')\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6cacd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(repeats, train, test, dropout_rate):\n",
    "    # paprameters\n",
    "    batch_size = 128\n",
    "    neurons = 32\n",
    "    nb_epochs = 250\n",
    "    prec_scores, recall_scores, auc_scores, losses = list(), list(), list(), list()\n",
    "    for r in range(repeats):\n",
    "        # fit the model\n",
    "        train_trimmed = train[len(train)%batch_size:]\n",
    "        lstm_model = fit_lstm(train_trimmed, batch_size, nb_epochs, neurons, dropout=dropout_rate)\n",
    "        # forecast the training set to set state\n",
    "        train_reshaped = train_trimmed[:,:-5].reshape(len(train_trimmed), 1, -1)\n",
    "        lstm_model.predict(train_reshaped, batch_size=batch_size, verbose=0)\n",
    "        # now forecast the test set\n",
    "        test_trimmed = test[:-(len(test) % batch_size)] if (len(test)%batch_size) != 0 else test\n",
    "        test_reshaped = test_trimmed[:,:-5].reshape(len(test_trimmed), 1, -1)\n",
    "        loss, prec, rec, auc = lstm_model.evaluate(test_reshaped, test_trimmed[:,-5:], batch_size)\n",
    "        print('%d) Test metrics - loss: %.3f, prec: %.3f, rec: %.3f, roc_auc: %.3f' %(r+1, loss, prec, rec, auc))\n",
    "        prec_scores.append(prec)\n",
    "        recall_scores.append(rec)\n",
    "        auc_scores.append(auc)\n",
    "        losses.append(loss)\n",
    "    return losses, prec_scores, recall_scores, auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9440169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/250.........11/250.........21/250.........31/250.........41/250.........51/250.........61/250.........71/250.........81/250.........91/250.........101/250.........111/250.........121/250.........131/250.........141/250.........151/250.........161/250.........171/250.........181/250.........191/250.........201/250.........211/250.........221/250.........231/250.........241/250.........\n",
      "11/11 [==============================] - 6s 10ms/step - loss: 1.5565 - precision_10: 0.3960 - recall_10: 0.3125 - auc: 0.7453\n",
      "1) Test metrics - loss: 1.557, prec: 0.396, rec: 0.312, roc_auc: 0.745\n",
      "1/250.........11/250.........21/250.........31/250.........41/250.........51/250.........61/250.........71/250.........81/250.........91/250.........101/250.........111/250.........121/250.........131/250.........141/250.........151/250.........161/250.........171/250.........181/250.........191/250.........201/250.........211/250.........221/250.........231/250.........241/250.........\n",
      "11/11 [==============================] - 5s 7ms/step - loss: 1.5529 - precision_11: 0.2925 - recall_11: 0.2138 - auc: 0.7212\n",
      "2) Test metrics - loss: 1.553, prec: 0.293, rec: 0.214, roc_auc: 0.721\n",
      "1/250.........11/250.........21/250.........31/250.........41/250.........51/250.........61/250.........71/250.........81/250.........91/250.........101/250.........111/250.........121/250.........131/250.........141/250.........151/250.........161/250.........171/250.........181/250.........191/250.........201/250.........211/250.........221/250.........231/250.........241/250.........\n",
      "11/11 [==============================] - 5s 7ms/step - loss: 1.5841 - precision_12: 0.3439 - recall_12: 0.2244 - auc: 0.7174\n",
      "3) Test metrics - loss: 1.584, prec: 0.344, rec: 0.224, roc_auc: 0.717\n",
      "1/250.........11/250.........21/250.........31/250.........41/250.........51/250.........61/250.........71/250.........81/250.........91/250.........101/250.........111/250.........121/250.........131/250.........141/250.........151/250.........161/250.........171/250.........181/250.........191/250.........201/250.........211/250.........221/250.........231/250.........241/250.........\n",
      "11/11 [==============================] - 6s 7ms/step - loss: 1.4868 - precision_13: 0.2660 - recall_13: 0.1385 - auc: 0.7051\n",
      "4) Test metrics - loss: 1.487, prec: 0.266, rec: 0.138, roc_auc: 0.705\n",
      "1/250.........11/250.........21/250.........31/250.........41/250.........51/250.........61/250.........71/250.........81/250.........91/250.........101/250.........111/250.........121/250.........131/250.........141/250.........151/250.........161/250.........171/250.........181/250.........191/250.........201/250.........211/250.........221/250.........231/250.........241/250.........\n",
      "11/11 [==============================] - 6s 7ms/step - loss: 1.9443 - precision_14: 0.2860 - recall_14: 0.2216 - auc: 0.6821\n",
      "5) Test metrics - loss: 1.944, prec: 0.286, rec: 0.222, roc_auc: 0.682\n",
      "1/250.........11/250.........21/250.........31/250.........41/250.........51/250.........61/250.........71/250.........81/250.........91/250.........101/250.........111/250.........121/250.........131/250.........141/250.........151/250.........161/250.........171/250.........181/250.........191/250.........201/250.........211/250.........221/250.........231/250.........241/250.........\n",
      "11/11 [==============================] - 5s 7ms/step - loss: 1.5742 - precision_15: 0.3388 - recall_15: 0.2045 - auc: 0.7242\n",
      "6) Test metrics - loss: 1.574, prec: 0.339, rec: 0.205, roc_auc: 0.724\n",
      "1/250.........11/250.........21/250.........31/250.........41/250.........51/250.........61/250.........71/250.........81/250.........91/250.........101/250.........111/250.........121/250.........131/250.........141/250.........151/250.........161/250.........171/250.........181/250.........191/250.........201/250.........211/250.........221/250......"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23676\\2413365601.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maucs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maucs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecisions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23676\\2442308051.py\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(repeats, train, test, dropout_rate)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtrain_trimmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mlstm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m# forecast the training set to set state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_trimmed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_trimmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23676\\2925511646.py\u001b[0m in \u001b[0;36mfit_lstm\u001b[1;34m(train, batch_size, nb_epoch, neurons, dropout, label_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                 _r=1):\n\u001b[0;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2454\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2456\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1861\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropouts = [.05, .1, .2, .3, .5]\n",
    "repeats = 15\n",
    "losses, precisions, recalls, aucs = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "for d in dropouts:\n",
    "    losses[str(d)], precisions[str(d)], recalls[str(d)], aucs[str(d)] = experiment(repeats, train, test, d)\n",
    "print(losses.describe())\n",
    "print(precisions.describe())\n",
    "print(recalls.describe())\n",
    "print(aucs.describe())\n",
    "# save a box plot\n",
    "fig, ax = plt.subplots(2, 2, sharex=True)\n",
    "losses.boxplot(ax=[0][0])\n",
    "precisions.boxplot(ax=ax[0][1])\n",
    "recalls.boxplot(ax=ax[1][0])\n",
    "aucs.boxplot(ax=ax[1][1])\n",
    "plt.savefig('boxplot_dropouts.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

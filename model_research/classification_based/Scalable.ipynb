{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea09cb0b",
   "metadata": {},
   "source": [
    "# Predicting the direction of the Stock Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caeb29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, roc_auc_score, multilabel_confusion_matrix\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a36a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    def __init__(self, ticker):\n",
    "        self.model_name = ticker + '-model'\n",
    "        self.create_model(ticker)\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def create_model(self, ticker):\n",
    "        filename = ''\n",
    "        for file in os.listdir('../../data'):\n",
    "            if file.split('_')[0] == ticker:\n",
    "                filename = file\n",
    "        filename = '../../data/' + filename\n",
    "        _, self.train, self.test = self.prep_data(filename)\n",
    "        self.batches = 32\n",
    "        neurons = 32\n",
    "        nb_epochs = 100\n",
    "        train_trimmed = self.train[len(self.train)%self.batches:]\n",
    "        test_trimmed = self.test[:-(len(self.test) % self.batches)] if len(self.test) % self.batches != 0 else self.test\n",
    "        for file in os.listdir('../models/'):\n",
    "            if file.split('-')[0] == ticker:\n",
    "                self.model = load_model('../models/' + file)\n",
    "                return\n",
    "        self.compile_model()\n",
    "        self.fit_lstm(train_trimmed, nb_epochs, neurons)\n",
    "        self.model.save('../models/' + self.model_name + '.h5')\n",
    "        return\n",
    "        \n",
    "    def retrain(self, batches = 32, neurons = 32, nb_epochs = 100):\n",
    "        self.batches = batches\n",
    "        train_trimmed = self.train[len(self.train)%self.batches:]\n",
    "        self.model = None\n",
    "        self.fit_lstm(train_trimmed, nb_epochs, neurons)\n",
    "        print(self.model.summary())\n",
    "        self.model.save('../models/' + self.model_name + '.h5')\n",
    "    \n",
    "    def evaluate(self, batches = 32):\n",
    "        self.model.reset_states()\n",
    "        train_trimmed = self.train[len(self.train)%batches:]\n",
    "        test_trimmed = self.test[:-(len(self.test) % batches)] if len(self.test) % batches != 0 else self.test\n",
    "        # forecast the training set to set state\n",
    "        train_reshaped = train_trimmed[:,:-5].reshape(len(train_trimmed), 1, -1)\n",
    "        self.model.predict(train_reshaped, batch_size=batches, verbose=0)\n",
    "        # now forecast the test set\n",
    "        test_reshaped = test_trimmed[:,:-5].reshape(len(test_trimmed), 1, -1)\n",
    "        ytrue = test_trimmed[:,-5:]\n",
    "        yhat = self.model.predict(test_reshaped, batch_size=batches, verbose=0)\n",
    "        labels = 'horrid poor neutral good great'.split()\n",
    "        pred_class = np.argmax(yhat, axis=1)\n",
    "        preds = np.zeros((len(pred_class), 5))\n",
    "        preds[np.arange(pred_class.size), pred_class] = 1\n",
    "        print(classification_report(ytrue, preds, target_names=labels))\n",
    "\n",
    "    def fit_lstm(self, train, nb_epoch, neurons, label_size=5):\n",
    "        X, y = train[:,:-label_size], train[:,-label_size:]\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        # prepare a model\n",
    "        if not self.model or isinstance(self.model, Sequential):\n",
    "            self.compile_model(neurons)\n",
    "        \n",
    "        for i in range(nb_epoch):\n",
    "            if not i % 10:\n",
    "                print('%d/%d' % (i+1, nb_epoch), end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "            self.model.fit(X, y, epochs=1, batch_size=self.batches, verbose=0, shuffle=False)\n",
    "            self.model.reset_states()\n",
    "        print()\n",
    "    \n",
    "    def predict(self):\n",
    "        self.model.reset_states()\n",
    "        test_trimmed = self.test[(len(self.test) % self.batches):] if len(self.test) % self.batches != 0 else self.test\n",
    "        test_reshaped = test_trimmed[:,:-5].reshape(len(test_trimmed), 1, -1)\n",
    "        yhat = self.model.predict(test_reshaped, batch_size=self.batches)\n",
    "        print('five days from %s probabilities:' % (self.data.columns[-1]))\n",
    "        print('fall:%.3f, dip:%.3f, stay:%.3f, rise:%.3f, rocket:%.3f'\n",
    "              %(yhat[-1,0], yhat[-1,1], yhat[-1,2], yhat[-1,3], yhat[-1,4]))\n",
    "    \n",
    "    def compile_model(self, neurons = 32, label_size=5):\n",
    "        batch_size = self.batches\n",
    "        X= self.train[:,:-label_size]\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(neurons,\n",
    "                       batch_input_shape=(batch_size, X.shape[1], X.shape[2]),\n",
    "                       stateful=True,\n",
    "                       return_sequences=True,\n",
    "                       dropout=.2))\n",
    "        self.model.add(LSTM(neurons,\n",
    "                       batch_input_shape=(batch_size, X.shape[1], X.shape[2]),\n",
    "                       stateful=True,\n",
    "                       dropout=.2))\n",
    "        self.model.add(Dense(5, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Precision(),\n",
    "                                                                                       tf.keras.metrics.Recall(),\n",
    "                                                                                       'AUC'])\n",
    "    \n",
    "    def read_data(self, filename):\n",
    "        # read in the data\n",
    "        data = pd.read_csv(filename)\n",
    "        data = data.rename(str.lower, axis=1)\n",
    "        data = data.rename(mapper={'adj close': 'adj_close'}, axis=1)\n",
    "        data.date = pd.to_datetime(data.date)\n",
    "        data = data.set_index('date')\n",
    "        return data\n",
    "\n",
    "    # helper function for one of our technical indicators\n",
    "    def _calc_rsi(self, df, periods = 14, ema = True):\n",
    "        \"\"\"\n",
    "        Returns a pd.Series with the relative strength index.\n",
    "        \"\"\"\n",
    "        close_delta = df.adj_close.diff()\n",
    "\n",
    "        # Make two series: one for lower closes and one for higher closes\n",
    "        up = close_delta.clip(lower=0)\n",
    "        down = -1 * close_delta.clip(upper=0)\n",
    "\n",
    "        if ema == True:\n",
    "            # Use exponential moving average\n",
    "            ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "            ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        else:\n",
    "            # Use simple moving average\n",
    "            ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "            ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "\n",
    "        rsi = ma_up / ma_down\n",
    "        rsi = 100 - (100/(1 + rsi))\n",
    "        return rsi\n",
    "    \n",
    "    def add_features(self, df):\n",
    "        # accumulation/Distribution line\n",
    "        mult = ((df.close - df.low) - (df.high - df.close)) / (df.high - df.low)\n",
    "        MFVolume = mult * df.volume\n",
    "        accum_dist_indicator = MFVolume.cumsum()\n",
    "        ret_df = pd.concat([df, accum_dist_indicator], axis=1)\n",
    "        ret_df = ret_df.rename(mapper={0:'accum_dist_indicator'}, axis=1)\n",
    "\n",
    "        #MACD\n",
    "        EMA_12 = df.adj_close.ewm(span=12, adjust=False).mean()\n",
    "        EMA_26 = df.adj_close.ewm(span=26, adjust=False).mean()\n",
    "        macd = EMA_12 - EMA_26\n",
    "        signal = macd.ewm(span=9, adjust=False).mean()\n",
    "        ret_df = pd.concat([ret_df, macd.rename('macd'), signal.rename('signal_macd')], axis=1)\n",
    "\n",
    "        #RSI\n",
    "        rsi = self._calc_rsi(df)\n",
    "        ret_df = pd.concat([ret_df, rsi.rename('rsi')], axis=1)\n",
    "\n",
    "        return ret_df\n",
    "\n",
    "    def _class_separation(self, x):\n",
    "        if x < -.05:\n",
    "            return -2\n",
    "        elif x < -.005:\n",
    "            return -1\n",
    "        elif x > .05:\n",
    "            return 2\n",
    "        elif x > .005:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def prep_data(self, filename, lookback=14):\n",
    "        temp = np.array([0,1,2,-1,-2]).reshape(-1,1)\n",
    "        # read in the data\n",
    "        data = self.read_data(filename)\n",
    "        self.data = data\n",
    "        # add technical indicators as features\n",
    "        data = self.add_features(data)\n",
    "        # frame as an RNN problem\n",
    "        data = self.series_to_supervised(data, lookback, 5, data.columns, data.index, [4])\n",
    "        # only keep the predictive columns I care about\n",
    "        data = data.drop(data.columns[-5:-1], axis=1)\n",
    "        labels = (data['adj_close(t+4)'] - data['adj_close(t-1)']) / data['adj_close(t-1)']\n",
    "        labels = labels.apply(self._class_separation)\n",
    "        data = data.drop(data.columns[-1], axis=1)\n",
    "        data = pd.concat([data, labels.rename('labels')], axis=1)\n",
    "        data_values = data.values\n",
    "        train, test = self.split_data(data_values, .2)\n",
    "        scaler, train_scaled, test_scaled = self.scale(train[:,:-1], test[:,:-1])\n",
    "        ohe = OneHotEncoder(sparse=False).fit(temp)\n",
    "        train_scaled = np.append(train_scaled, ohe.transform(train[:,-1].reshape((-1,1))), axis=1)\n",
    "        test_scaled = np.append(test_scaled, ohe.transform(test[:,-1].reshape((-1,1))), axis=1)\n",
    "        return scaler, train_scaled, test_scaled\n",
    "    \n",
    "    def series_to_supervised(self, data, n_in=1, n_out=1, col_names = [], indicies = [], preds = [], dropnan=True):\n",
    "        '''\n",
    "        Convert a time series to a supervised learning dataset\n",
    "        Args:\n",
    "            data -> time series to convert as a list or numpy array\n",
    "            n_in -> number of lag observations as input (X)\n",
    "            n_out -> number of observations as output (y)\n",
    "            col_names -> names of the columns\n",
    "            indicies -> list of the indicies\n",
    "            preds -> list of column indicies to determine which variables to predict\n",
    "            dropnan -> flag of whether to drop the rows with NaN\n",
    "        Returns:\n",
    "            Pandas DataFrame of series framed for supervised learning\n",
    "        '''\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        df = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('%s(t-%d)' % (col_names[j], i)) for j in range(n_vars)]\n",
    "        # forecast sequence\n",
    "        for i in range(0, n_out):\n",
    "            cols.append(df[col_names[preds]].shift(-i))\n",
    "            if i==0:\n",
    "                names += [('%s(t)' % (col_names[j])) for j in preds]\n",
    "            else:\n",
    "                names += [('%s(t+%d)' % (col_names[j], i)) for j in preds]\n",
    "        # putting it together\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        agg.index = indicies\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "        return agg\n",
    "\n",
    "    def scale(self, in_train, in_test, with_labels=False):\n",
    "        '''\n",
    "        Rescales the train and test sets\n",
    "        Args:\n",
    "            train -> numpy array of the training data\n",
    "            test -> numpy array of the test data\n",
    "            with_labels -> if set to true will cut off last column before scaling,\n",
    "                reattached after scaling\n",
    "        Returns:\n",
    "            scaler -> the scaler object for transforming\n",
    "            train_scaled -> a rescaled version of the train data\n",
    "            test_scaled -> a rescaled version of the test data\n",
    "        '''\n",
    "        train = in_train\n",
    "        test = in_test\n",
    "        if with_labels:\n",
    "            train_labels = train[:,-1]\n",
    "            train = train[:,:-1]\n",
    "            test_labels = test[:,-1]\n",
    "            test = test[:,:-1]\n",
    "        # scale train and test to [-1,1]\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        scaler = scaler.fit(train)\n",
    "        # transform train\n",
    "        train = train.reshape(train.shape[0], train.shape[1])\n",
    "        train_scaled = scaler.transform(train)\n",
    "        # transform test\n",
    "        test = test.reshape(test.shape[0], test.shape[1])\n",
    "        test_scaled = scaler.transform(test)\n",
    "        if with_labels:\n",
    "            train_scaled.append(train_labels, axis=1)\n",
    "            test_scaled.append(test_labels, axis=1)\n",
    "        return scaler, train_scaled, test_scaled\n",
    "\n",
    "    def split_data(self, data, test_percent):\n",
    "        '''\n",
    "        Splits the data by percentage amount\n",
    "        Returns: train, test\n",
    "        '''\n",
    "        split_val = int(len(data) * (1 - test_percent))\n",
    "        train, test = data[:split_val], data[split_val:]\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c18637a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (32, 1, 32)               22144     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (32, 32)                  8320      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (32, 5)                   165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,629\n",
      "Trainable params: 30,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "my_model = ModelLoader('SPY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "342f658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      horrid       0.00      0.00      0.00        42\n",
      "        poor       0.24      0.23      0.24       349\n",
      "     neutral       0.28      0.56      0.38       351\n",
      "        good       0.46      0.27      0.34       700\n",
      "       great       0.05      0.03      0.04        30\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      1472\n",
      "   macro avg       0.21      0.22      0.20      1472\n",
      "weighted avg       0.35      0.32      0.31      1472\n",
      " samples avg       0.32      0.32      0.32      1472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97adf087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100.........11/100.........21/100.........31/100.........41/100.........51/100.........61/100.........71/100.........81/100.........91/100.........\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (32, 1, 32)               22144     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (32, 32)                  8320      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (32, 5)                   165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,629\n",
      "Trainable params: 30,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "my_model.retrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da39541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 3ms/step\n",
      "five days from volume probabilities:\n",
      "fall:0.172, dip:0.149, stay:0.299, rise:0.137, rocket:0.243\n"
     ]
    }
   ],
   "source": [
    "my_model.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
